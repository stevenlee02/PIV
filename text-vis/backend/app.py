import os, json, re
from typing import List, Dict
from fastapi import FastAPI, UploadFile
from fastapi.middleware.cors import CORSMiddleware
import spacy
import networkx as nx
from collections import defaultdict
from openai import OpenAI

# ---------------- åˆå§‹åŒ– ----------------
app = FastAPI()
nlp = spacy.load("en_core_web_sm")
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:8000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³›åŒ–é»‘åå• ç”¨äºclean_name
BLACKLIST = {
    "project gutenberg", "gutenberg", "ebook", "e-text", "etext", "epub", "html", "ascii", "txt",
    "produced by", "transcribed by", "distributed proofreaders", "formatting", "release date",
    "license", "limited warranty", "liability", "copyright", "literary archive", "archive",
    "foundation", "internal revenue", "irs", "ein", "government",
    "preface", "editor", "translator", "illustrator", "publisher", "chapter", "send", "send:",
    "general information", "terms", "warranty", "possibility", "punitive", "liable", "direct",
    # å¸¸è§ä½œè€…
    "o henry", "o. henry", "michael s hart", "project gutenberg literary archive foundation",
    "mark twain", "jane austen", "charles dickens", "william shakespeare",
    # æ¯”å–»orå†å²äººç‰©
    "solomon", "king solomon", "caesar", "napoleon", "homer"
}

# å‰ç¼€é›†åˆ
HONORIFICS = {"mr", "mrs", "ms", "miss", "dr", "mme", "mlle", "sir", "lady", "lord", "master", "mx"}

# ç”¨äºåœ¨promptä¸­æ’é™¤çš„è¯
SKIP_WORDS = ["copyright", "edition", "chapter", "preface", "project", "release", "translator", "gutenberg"]

def split_text_by_sentence(text):
    """æŒ‰å¥å­æ‹†åˆ†"""
    text = re.sub(r'\s+', ' ', text)
    doc = nlp(text)
    return [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 20]

def extract_persons(chunk):
    """æå–å¥å­ä¸­çš„äººå"""
    doc = nlp(chunk)
    return [ent.text.strip() for ent in doc.ents if ent.label_ == "PERSON"]


# å¤„ç†åå­—
def clean_name(name: str) -> str:
    if not name:
        return ""

    name = re.sub(r"[\r\n]+", " ", name)
    name = re.sub(r"\s+", " ", name).strip()
    name = re.sub(r"[\[\(\{].*?[\]\)\}]", "", name)
    name = re.sub(r"[^A-Za-z\s']", "", name).strip()

    if not name:
        return ""

    parts = name.split()
    if not parts:
        return ""

    # honorific
    if parts[0].lower().strip(".") in HONORIFICS and len(parts) == 1:
        return ""

    # é‡å»ºåå­—
    name_cleaned = " ".join(p.capitalize() for p in parts)

    # å»æ‰åç¼€æ®‹ç•™çš„è¯ ä¾‹å¦‚ --but said
    name_cleaned = re.sub(r"\b(but|said|says|then|also)\b", "", name_cleaned, flags=re.IGNORECASE).strip()

    # é»‘åå•è¿‡æ»¤
    lname = name_cleaned.lower()
    for bad in BLACKLIST:
        if bad in lname:
            return ""

    return name_cleaned



def clean_illustrations(text: str) -> str:
    """
    Remove Gutenberg illustration blocks such as:
    [Illustration: ...]
    [Illustration ...]
    [_Copyright 1894 by ...]
    """

    text = re.sub(
        r"\[.*?illustration.*?\]",
        "",
        text,
        flags=re.IGNORECASE | re.DOTALL
    )

    text = re.sub(
        r"\[_copyright.*?\]",
        "",
        text,
        flags=re.IGNORECASE | re.DOTALL
    )

    """text = re.sub(r"\[[^\]]{0,200}\]", " ", text)
    # Normalize whitespace
    text = re.sub(r"\n\s+\n", "\n\n", text)
    """

    return text


# ---------------- ä¸»åˆ†ææ¥å£ ----------------
@app.post("/analyze")
async def analyze(file: UploadFile):
    text = (await file.read()).decode("utf-8", errors="ignore")
    # åˆ æ‰Gutenberg illustration
    text = clean_illustrations(text)

    skip_words = [
        "copyright", "edition", "chapter", "preface",
        "project", "release", "translator", "gutenberg"
    ]
    
    print("ğŸ”¹ Splitting sentences and extracting names...")
    sentences = split_text_by_sentence(text)
    all_names, sent_persons = [], []
    sent_persons = []  

    for sent in sentences:
        raw_persons = extract_persons(sent)  
        persons = []
        for p in raw_persons:
            p_clean = clean_name(p)
            if p_clean:
                persons.append(p_clean)
        if persons:
            sent_persons.append(persons)
            all_names.extend(persons)
        else:
            sent_persons.append([])


    unique_names = sorted(set(all_names))[:200]
    print(f"Extracted {len(unique_names)} candidate names.")

    prompt = f"""
You are an expert in literary text analysis.

You are given a list of PERSON entities automatically extracted from a novel.
They may include:
- Character names (like "Elizabeth Bennet", "Mr. Darcy")
- Nicknames (like "Lizzy")
- Non-character entities (like "Jane Austen", "Project Gutenberg")

Here is the extracted list (max 200 entries):
{json.dumps(unique_names, ensure_ascii=False)}

Your job:
1. Identify which names correspond to fictional characters from the novel.
2. Merge all name variants that refer to the same character (for example: "Darcy", "Mr. Darcy", "Fitzwilliam Darcy" â†’ "Mr. Darcy").
3. Exclude any of the following:
   - Author names
   - Publishers, editors, illustrators, and translators
   - Non-human entities or locations
   - Words like "Chapter", "Copyright", "Project Gutenberg"
4. Output only valid JSON in this exact structure:

{{ "Canonical Character Name": ["variant1", "variant2", "variant3"] }}

Rules:
- Output must be a single valid JSON object.
- No markdown, no explanations, no comments, no backticks.
- Keep only names of fictional characters that appear within the story.
"""

    # ---------------- GPT è§’è‰²èšåˆ ----------------
    try:
        print("ğŸ”¹ Calling GPT model for name normalization...")
        resp = client.responses.create(
            model="gpt-4o-mini",
            temperature=0,
            input=[
                {"role": "system", "content": "Return valid JSON only."},
                {"role": "user", "content": prompt},
            ],
        )

        mapping_text = ""
        if hasattr(resp, "output_text") and resp.output_text:
            mapping_text = resp.output_text.strip()
        else:
            # Fallback: try to extract from resp.output or resp.get("output", ...)
            try:
                # resp.output is often a list of dicts with 'content' items
                if isinstance(resp.output, list):
                    parts = []
                    for item in resp.output:
                        # try a few likely keys
                        if isinstance(item, dict):
                            # some SDKs: item["content"][0]["text"]
                            content = item.get("content") or item.get("text")
                            if isinstance(content, list):
                                for c in content:
                                    if isinstance(c, dict) and "text" in c:
                                        parts.append(c["text"])
                                    elif isinstance(c, str):
                                        parts.append(c)
                            elif isinstance(content, str):
                                parts.append(content)
                    mapping_text = "\n".join(parts).strip()
                else:
                    mapping_text = str(resp)
            except Exception:
                mapping_text = str(resp)
        print("ğŸ”¹ GPT raw output preview (first 400 chars):")
        print(mapping_text[:400])

        # è§£æ JSON
        cleaned = mapping_text
        if cleaned.startswith("```"):
            cleaned = re.sub(r"^```(?:json)?\s*", "", cleaned)
            cleaned = re.sub(r"\s*```$", "", cleaned)
        mapping = json.loads(cleaned)
        print("GPT mapping parsed successfully.")
    except Exception as e:
        print("ğŸ”¸ GPT fallback: parsing failed or GPT error:", e)
        # å¦‚æœGPTå¤±è´¥ æ¯ä¸ªåå­—æ˜ å°„åˆ°è‡ªå·±
        mapping = {name: [name] for name in unique_names}

    # ---------------- å»ºç«‹å˜ä½“ -> canonical æ˜ å°„ï¼ˆä¿è¯å®Œæ•´ï¼‰ ----------------
    variant_to_canon = {}

    # å†™å…¥GPTæä¾›çš„mapping åŒ…æ‹¬canonicalæœ¬èº«
    for canon, variants in mapping.items():
        canon_clean = clean_name(canon)
        if not canon_clean:
            continue
        # å¿½ç•¥å«skip_wordsçš„canonical
        if any(k in canon_clean.lower() for k in skip_words):
            continue
        # æŠŠcanonicalæ˜ å°„åˆ°è‡ªå·± ä¿è¯canonicalå‡ºç°åœ¨mapping
        variant_to_canon[canon_clean] = canon_clean
        # æŠŠå˜ä½“æ˜ å°„åˆ°canonical
        if isinstance(variants, list):
            for v in variants:
                v_clean = clean_name(v)
                if v_clean and not any(k in v_clean.lower() for k in skip_words):
                    variant_to_canon[v_clean] = canon_clean
    
    # åªä»ç°æœ‰çš„åå­—ä¸­æå–éƒ¨åˆ†åŒ¹é… ä¸åˆ›å»ºæ–°åå­— 
    def enhance_mapping(variant_to_canon, all_extracted_names):
        enhanced_mapping = variant_to_canon.copy()
        canonicals = list(set(enhanced_mapping.values()))
        all_names_set = set(all_extracted_names)  # æ‰€æœ‰å®é™…æå–åˆ°çš„åå­—
    
        # ä¸ºæ¯ä¸ªå·²å­˜åœ¨çš„åå­—å˜ä½“ æ£€æŸ¥å…¶éƒ¨åˆ†æ˜¯å¦ä¹Ÿåœ¨æå–çš„åå­—åˆ—è¡¨ä¸­
        for existing_variant in list(enhanced_mapping.keys()):
            parts = existing_variant.split()
            if len(parts) <= 1:
                continue
            
            # æ£€æŸ¥æ¯ä¸ªéƒ¨åˆ†æ˜¯å¦ç‹¬ç«‹å­˜åœ¨äºæå–çš„åå­—ä¸­
            for part in parts:
                if (len(part) > 2 and  # é¿å…å¤ªçŸ­çš„åŒ¹é…
                    part in all_names_set and  
                    part not in enhanced_mapping and
                    not any(k in part.lower() for k in skip_words)):
                    # å°†è¿™ä¸ªéƒ¨åˆ†æ˜ å°„åˆ°åŒä¸€ä¸ª canonical
                    enhanced_mapping[part] = enhanced_mapping[existing_variant]
    
        return enhanced_mapping

    variant_to_canon = enhance_mapping(variant_to_canon, unique_names)

    # å¯¹unique_namesä¸­ä»»ä½•æœªè¢«GPTæ˜ å°„çš„åå­—åšidentityæ˜ å°„
    for n in unique_names:
        n_clean = clean_name(n)
        if not n_clean:
            continue

        # å¦‚æœå°¾éƒ¨æ˜¯så°è¯•å»æ‰å†åŒ¹é…
        if n_clean.lower().endswith("s"):
            singular = n_clean[:-1]
            if singular in variant_to_canon:
                variant_to_canon[n_clean] = variant_to_canon[singular]
                continue

        # å¦‚æœåå­—å·²ç»åœ¨æ˜ å°„ä¸­å°±è·³è¿‡
        if n_clean in variant_to_canon:
            continue
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ˜ å°„åˆ°ç°æœ‰çš„ canonical
        matched = False
        for canon in set(variant_to_canon.values()):
            # æ£€æŸ¥n_cleanæ˜¯å¦æ˜¯canonçš„ä¸€éƒ¨åˆ† æˆ–è€…canonæ˜¯n_cleançš„ä¸€éƒ¨åˆ†
            if (n_clean in canon.split() or canon in n_clean.split() or
                n_clean == canon):
                variant_to_canon[n_clean] = canon
                matched = True
                break
    
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•ç°æœ‰çš„ åšidentityæ˜ å°„
        if not matched and not any(k in n_clean.lower() for k in skip_words):
            variant_to_canon[n_clean] = n_clean

    
    # ---------------- ç”¨ canonical åç§°æ„å»ºå…±ç°ç½‘ç»œ ----------------
    def build_cooccurrence_network(sentences, sent_persons, variant_to_canon):
        G = nx.Graph()
        cooccurrence_texts = defaultdict(list)

        for sent, persons in zip(sentences, sent_persons):
            # å°†å¥å­ä¸­æå–çš„æ¯ä¸ªåå­—æ›¿æ¢æˆcanonical è‹¥æ²¡æœ‰canonicalåˆ™è·³è¿‡
            canon_list = []
            for p in persons:
                p_clean = clean_name(p)
                if not p_clean:
                    continue
                canon_name = variant_to_canon.get(p_clean)
                if canon_name:
                    canon_list.append(canon_name)
            canon_list = list(set(canon_list))  # å»é‡

            if not canon_list:
                continue

            # å¢åŠ èŠ‚ç‚¹è®¡æ•° ä½¿ç”¨canonical name
            for a in canon_list:
                if a in G.nodes:
                    G.nodes[a]["count"] += 1
                else:
                    G.add_node(a, count=1)
            # å¢åŠ è¾¹å¹¶ä¿å­˜ä¸Šä¸‹æ–‡å¥å­
            for i in range(len(canon_list)):
                for j in range(i + 1, len(canon_list)):
                    a, b = canon_list[i], canon_list[j]
                    if G.has_edge(a, b):
                        G[a][b]["weight"] += 1
                    else:
                        G.add_edge(a, b, weight=1)
                    # use sorted key so "A|B" and "B|A" map same
                    key = "|".join(sorted([a, b]))
                    if len(cooccurrence_texts[key]) < 5:  # é™åˆ¶ä¸Šä¸‹æ–‡æ¡æ•° 
                        ###è¿™é‡Œæ˜¯å¦éœ€è¦åŒºåˆ†é•¿æ–‡æœ¬å’ŒçŸ­æ–‡æœ¬ å¯¹äºçŸ­æ–‡æœ¬å¦‚æœæ˜¯è¿‡é•¿çš„ä¸Šä¸‹æ–‡ å¯¼è‡´ä¸åŒäººç‰©è¢«èšåˆåˆ°ä¸€èµ·
                        cooccurrence_texts[key].append(sent[:400])

        nodes_to_keep = [n for n in G.nodes if G.nodes[n].get("count", 0) >= 5]
    
        # åˆ›å»ºå­å›¾ åªä¿ç•™å‡ºç°æ¬¡æ•°>=5çš„èŠ‚ç‚¹
        G_filtered = G.subgraph(nodes_to_keep).copy()
    
        all_nodes = list(G_filtered.nodes)
        node_set = set(all_nodes)


        # linksï¼šåªä¿ç•™ä¸¤ç«¯éƒ½åœ¨node_setçš„è¾¹
        links = []
        for u, v, data in G.edges(data=True):
            if u in node_set and v in node_set and data.get("weight", 0) > 0:
                links.append({"source": u, "target": v, "value": int(data["weight"])})

        # nodesï¼šè¾“å‡ºæ‰€æœ‰åœ¨node_setä¸­çš„èŠ‚ç‚¹ æŒ‰countæ’åºï¼ˆå¤§åˆ°å°ï¼‰
        nodes = [{"id": n, "value": int(G.nodes[n].get("count", 1))} for n in all_nodes]
        nodes = sorted(nodes, key=lambda x: x["value"], reverse=True)

        print(f"Final Network: {len(nodes)} nodes, {len(links)} edges.")
   
        filtered_contexts = {}
        for key, contexts in cooccurrence_texts.items():
            chars = key.split("|")
            if len(chars) == 2 and chars[0] in node_set and chars[1] in node_set:
                filtered_contexts[key] = contexts

        return {"nodes": nodes, "links": links, "contexts": dict(cooccurrence_texts)}
    
    result = build_cooccurrence_network(sentences, sent_persons, variant_to_canon)
    return result

@app.get("/ping")
async def ping():
    return {"message": "pong", "key_loaded": bool(os.getenv("OPENAI_API_KEY"))}
